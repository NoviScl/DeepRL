第二章 强化学习建模问题：马尔科夫决策过程
===
本章内容：
- 字符串图和我们的教学方法
- PyTorch深度学习框架
- 解决多臂赌博机问题
- 平衡探索与开发
- 将问题建模为马尔可夫决策过程（MDP）
- 实现神经网络来解决广告选择问题

字符串图和我们的教学方法
---
本章介绍了所有强化学习中的一些最基本的概念，并将作为本书其余部分的基础。 但在我们开始讨论之前，我们首先要讨论一下我们将在本书中使用的一些反复出现的教学方法，最值得注意的是，我们在上一章提到的字符串图。

根据我们的经验，当大多数人试图教授复杂的东西时，他们倾向于按照主题本身的相反顺序来教授它。 他们会给你一堆定义，术语，描述和定理，然后他们会说，“太好了，既然我们已经涵盖了所有的理论，让我们回顾一些实践问题。”我们认为，这正是 应该呈现事物的相反顺序。 大多数好的想法都是作为解决世界上真正问题的解决方案，或者至少是想象中出现的问题。 问题解决者会遇到潜在的解决方案，测试它，改进它，然后最终得到正式化并可能数学化。 所有的术语和定义都是在考虑到问题的解决方案之后出现的。

我们认为，当你取代那个正在思考如何解决某个特定问题的创意制造者时，学习一些东西是最有动力和最有效的。 只有一旦解决方案的直觉结晶，它才能保证形式化，这确实是必要的，以确定其正确性并忠实地将其传达给该领域的其他人。 有一种强烈的冲动要求采用这种反向的时间顺序教学模式，但我们会尽力抵制它并随时开发这个主题。 本着这种精神，我们将根据需要引入新的术语，定义和数学符号。 例如，我们将使用这样的“标注”框：
> 神经网络是一种由多个“层”组成的机器学习模型，它们执行矩阵向量乘法，然后应用非线性“激活”函数。 神经网络的矩阵是模型的可学习参数，通常称为神经网络的“权重”。

每个术语只会看到一次这些标注框，但我们通常会在文本中以不同的方式重复这些标注，以确保您真正理解它并记住它。 这是一门关于强化学习的课程，而不是教科书或参考书，所以当我们认为重要的事情要记住时，我们不会回避重复自己。

每当我们需要引入一些数学时，我们通常会使用一个显示数学的框和一个相同基础概念的伪Python版本。 有时在代码或数学方面更容易思考，无论如何，我们认为熟悉这两者是很好的。 作为一个非常简单的例子，如果我们引入一条线的方程式，我们会这样做：
```
Math
y=mx+b
```
```
pseudocode
def line(x,m,b):
	return m*x+b
```
我们还包括大量的内联代码（简短代码段）和代码清单（更长的代码示例）以及完整项目的代码。 本书中的所有代码都在Jupyter笔记本中提供，该笔记本按照本书[GitHub存储库](http://github.com/DeepReinforcementLearning/)

由于强化学习涉及许多相互关联的概念，这些概念在单词留下时会变得令人困惑，因此我们包含了许多不同种类的图表和图形。 我们使用的两种更重要的数字是字符串图和olog（“oh-log”）。 他们可能有奇怪的名字，但他们都是非常简单的想法，并且改编自类别理论，我们在第一章中提到的数学分支，他们倾向于使用大量图表来补充或取代传统的符号表示法。
当我们在第1章介绍强化学习的一般框架时，我们已经遇到过一个olog：

![图片](深度强化学习实战/Figure2.1.png)
这个想法是盒子包含名词或名词短语，而箭头用动词或动词短语标记。 它与典型的流程图略有不同，但它可以很容易地将olog图转换成英文散文，反之亦然，并且非常清楚箭头在功能上究竟是做什么的。

最后，字符串图（有时在其他来源中称为接线图）也是类似流的图，表示沿着字符串（即有向或无向箭头）的类型化数据流入过程（计算，函数，转换等） 用框表示。 字符串图和类似的流程图之间的重要区别在于，线上的所有数据都是明确键入的（例如，具有形状或简单浮点数的numpy数组）并且它们是完全组合的。 通过组合，我们的意思是我们可以以原则的方式“放大”或放出图表，以查看更大的更抽象的图片或深入到计算细节。

如果我们显示更高级别的描述，则流程框可能只标有一个单词或短语，表示发生的流程类型，但我们也可以显示该流程框的放大视图，显示所有它的内部细节，由它自己的子串和子过程组成。这些图表的组成特性也意味着我们可以将部分图表插入到其他图表中，形成更复杂的图表，只要所有字符串的类型兼容即可。
例如，这是一个神经网络的单层作为字符串图：
![图片](深度强化学习实战/Figure2.2.png)
从左到右阅读，我们看到一些类型为“n”的数据流入一个名为“神经网络层”的过程框，并产生类型为“m”的输出。由于神经网络通常将向量作为输入并将向量作为输出，这些类型分别指输入和输出向量的维度。也就是说，该神经网络层接受长度或维度为n的向量并产生维度为m的向量。对于某些神经网络层，n = m是可能的。这种“键入”字符串的方式是简化的，我们只有在明确了类型对上下文的意义时才会这样做。在其他情况下，我们可以使用诸如ℝ的数学符号来表示所有实数的集合，这在编程语言中基本上转换为浮点数。因此，对于维度为n的浮点数向量，我们可以像这样输入字符串：
![图片](深度强化学习实战/Figure2.3.png)
现在输入更丰富，我们不仅知道输入和输出向量的大小，我们知道它们是实数/浮点数。 虽然几乎总是这样，但有时我们可能会处理整数或二进制数。 在任何情况下，我们的处理盒“神经网络层”都留作黑盒子，除了将矢量转换为可能不同维度的另一个矢量这一事实外，我们不知道究竟发生了什么。 我们可以决定放大这个过程，看看具体发生了什么：
![图片](深度强化学习实战/Figure2.4.png)
现在我们可以看到原始流程框的内部，它由自己的一组子流程组成。 我们可以看到，我们的n维向量乘以维数n×m的矩阵，产生m维向量积。 然后该向量通过一个称为“ReLU”的过程，您可以将其识别为标准神经网络激活函数，即整流线性单元。 如果需要，我们可以继续放大ReLU子子流程。 因此，任何值得命名的字符串图表必须能够在任何抽象级别进行仔细检查并保持良好类型（意味着进入和退出进程的数据类型是兼容的并且有意义，例如应该生成排序的进程 列表不应该连接到任何级别的另一个期望整数的进程。

只要字符串输入良好，我们就可以将一堆进程串联到一个复杂的系统中。 这允许我们构建一次组件，并在类型匹配的任何地方重新使用它们。 在某种程度上，我们可能会描绘一个简单的两层递归神经网络（RNN），如下所示：
![图片](深度强化学习实战/Figure2.5.png)

所以这个RNN接收一个q向量并产生一个s向量。 但是，我们可以看到内部流程。 有两层，每层都看起来功能相同。 它们各自接收一个向量并生成一个向量，除了输出向量被复制并作为输入的一部分反馈到层过程中，因此重现。

字符串图是一种非常通用的图表类型; 除了绘制神经网络图之外，我们还可以用它们来描绘如何烤蛋糕。 一种特殊的字符串图是计算图。 计算图是一个字符串图，其中所有进程代表计算机可以执行的具体计算，或者可以用某些编程语言（如Python）描述。 如果您曾在TensorFlow的TensorBoard中看到计算图，那么这就是我们的意思。

解决多臂赌博机问题
---
好的，我们已准备好开始真正的强化学习问题，并学习解决这个问题所需的相关概念和技能。 但在我们过于花哨地构建AlphaGo之前，让我们首先考虑一个简单的问题（很容易转化为实际问题）。 假设你在赌场和一些老虎机的部分。 在你面前有10台老虎机连续闪亮的标志，上面写着“免费玩！最高支付10美元！” 哇，不错吧！ 好奇，你问其中一位员工这里发生了什么，看起来好得令人难以置信，她说：“这是真的，尽可能多地玩，它是免费的。每台老虎机都保证给你奖励 0美元和10美元。顺便说一句，保持这个，但这10台老虎机每个都有不同的平均支出，所以试着找出哪一个平均给出最多的奖励，你将赚到大量的现金！”

这是什么样的赌场？！谁在乎，让我们弄清楚如何赚到最多钱！哦顺便说一下，这是一个笑话：老虎机的另一个名字是什么？ ......一个单臂匪徒！得到它？它有一只手臂（杠杆），它通常会偷你的钱！嗯，我想我们可以把我们的情况称为10武装强盗问题，或更普遍的n武装强盗问题，其中n是老虎机的数量。虽然到目前为止这个问题听起来很奇怪，但后来我们会发现这些所谓的n型武装强盗（或多臂强盗）问题确实有一些非常实际的应用。

让我更正式地重申我们的问题。我们有n个可能的动作（这里n = 10），其中动作意味着拉动特定老虎机的手臂/杠杆，并且在该游戏的每个游戏（k）中我们可以选择单个杠杆拉动。采取行动后，我们将获得奖励$R_k$（在游戏中奖励k）。每个杠杆都有独特的支付概率分布（奖励）。例如，如果我们有10台老虎机并玩很多游戏，老虎机＃3可能会给出9美元的平均奖励，而老虎机＃1只能给出4美元的平均奖励。当然，由于每场比赛的奖励都是概率性的，所以杆＃1有可能在一场比赛中给我们9美元的奖励。但是如果我们玩很多游戏，我们预计普通老虎机＃1的奖励低于＃3。

换句话说，我们的策略应该是玩几次，选择不同的杠杆并观察我们对每个动作的奖励。 然后我们只想选择具有最大观察平均奖励的杠杆。 因此，我们需要一个基于我们之前的戏剧采取行动的预期奖励的概念。 我们将以数学方式称这个预期的奖励$Q_k$; 你给这个函数一个动作（假设我们正在玩`k`）并且它返回了采取该动作的预期奖励。从形式上看，
$$Q_k(a)=\frac {R_1+R_2+...+R_k}{k_a}$$
```python
def exp_reward(action,history):
	rewards_for_action = history[action]
	return sum(rewards_for_action)/len(rewards_for_action)
```

也就是说，对于动作`a`，游戏`k`的预期奖励是我们为采取行动`a`而获得的所有先前奖励的算术平均值。 因此，我们之前的行动和观察会影响我们未来的行动，我们甚至可能会说我们先前的一些行动会加强我们当前和未来的行动。 我们稍后再回过头来看看。 函数$Q_k$被称为`值函数`，因为它告诉我们某事物的价值。 特别是，它是一个actionvalue函数，因为它告诉我们采取特定操作的价值。 由于我们通常用符号`Q`表示该函数，因此它通常也称为`Q函数`。 我们稍后会回到值函数并给出更复杂的定义，但现在这已经足够了。

也就是说，行动中的预期奖励是我们为采取行动而获得的所有先前奖励的算术平均值。 因此，我们之前的行动和观察会影响我们未来的行动，我们甚至可能会说我们先前的一些行动会加强我们当前和未来的行动。 我们稍后再回过头来看看。 该函数称为值函数，因为它告诉我们某事物的价值。 特别是，它是一个动作值函数，因为它告诉我们采取特定动作的价值。 由于我们通常用符号Q表示该函数，因此它通常也称为Q函数。 我们稍后会回到值函数并给出更复杂的定义，但现在这已经足够了。

探索与开发。 当我们第一次开始游戏时，我们需要玩游戏并观察我们为各种机器获得的奖励，我们可以称之为策略探索，因为我们只是随机地探索我们行为的结果。 这与我们可以采用的一种称为剥削的不同策略形成对比，这意味着我们使用我们当前的知识来了解哪台机器似乎产生了最大的回报并且只是继续玩这台机器。 所以我们的整体战略需要包括一些开发（简单地根据我们目前所知的选择最佳杠杆）和一些探索（选择随机杠杆，以便我们可以了解更多）。 开发和探索的适当平衡对于最大化我们的回报非常重要。
那么我们怎么能想出一个算法来找出哪个老虎机的平均支出最高？ 那么，最简单的算法就是选择该等式为真的动作：
$$a=argmax_a(Q_k(a))$$
$$\forall a \in A_k$$
```python
def get_best_action(actions,history):
	exp_rewards=[exp_reward(action,history) for action in actions]
	return argmax(exp_rewards)
```
或者是下面这段python3代码：
```python
def get_best_action(actions):
    best_action = 0
    max_action_value = 0
    for i in range(len(actions)): #loop through all possible actions
        cur_action_value = get_action_value(actions[i]) #get the value of the current action
        if cur_action_value > max_action_value:
            best_action = i
            max_action_value = cur_action_value
    return best_action
```

> 该等式表明用于采取行动`α`的当前游戏`k`的预期奖励等于所有先前采取的行动的最大平均奖励。 
换句话说，我们在所有可能的操作上使用我们的上述奖励函数$Q_k$，并选择返回最大平均奖励的操作。 由于$Q_k$取决于我们之前的行动及其相关奖励的记录，因此该方法不会评估我们尚未探索过的行动。 因此，我们之前可能已经尝试过杠杆1和杠杆3，并注意到杠杆3给了我们更高的回报，但是通过这种方法，我们永远不会想到尝试另一个杠杆，比如说＃6，我们不知道，它实际上给出了 获得最高的平均奖励。 这种简单选择我们目前所知的最佳杠杆的方法被称为“贪婪”方法（或者我们讨论过，开发）

`ε-贪婪`的策略。 显然，我们需要对其他杠杆（老虎机）进行一些探索，以发现真正的最佳动作。 对上述算法的一个简单修改是将其改为`ε（epsilon）-greedy`算法，这样，用概率`ε`，我们将随机选择一个动作`a`，其余的时间（概率`1-ε`） 我们将根据我们目前对过去戏剧的了解选择最佳杠杆。 所以大部分时间我们都玩贪心，但有时我们冒一些风险并选择随机杠杆，看看会发生什么。 这当然会影响我们未来的贪婪行为。
用python解决这个问题
```python
import numpy as np
from scipy import stats
import random
import matplotlib.pyplot as plt
 
 
n = 10  # n个可选行为
arms = np.random.rand(n) #hidden probs associated with each arm
eps = 0.1  # 随机选择行为的概率是0.1
```

根据我们的赌场示例，我们将解决一个10个武装强盗问题，因此n = 10.我们还定义了一个长度为n的numpy数组，其中填充了随机浮点数，可以理解为概率。 我们选择为每个手臂/杠杆/老虎机实现我们的奖励概率分布的方式是：每个手臂都有一个概率，例如：0.7。 最高奖励是10美元。 我们将`for`循环设置为10，并且在每个步骤中，如果随机浮动小于手臂的概率，它将为奖励添加+1。 因此，在第一个循环中，它构成随机浮点（例如0.4）。 0.4小于0.7，所以`reward += 1`.在下一次迭代中，它组成另一个随机浮点数（例如0.6）也小于0.7，因此`reward+= 1`.这一直持续到我们完成10次迭代然后我们 返回最终的总奖励，可以是0到10之间的任何值。当手臂概率为0.7时，这样做到无穷大的平均奖励将是7，但在任何单一游戏中，它可能或多或少。

```python
def reward(prob, n=10):  # 我的理解是每个老虎机假设执行10次
    reward = 0;
    for i in range(n):
        if random.random() < prob:
            reward += 1
    return reward
```
我们定义的下一个函数是选择最优手臂的贪婪策略。这个函数将会输入一个存储单元为元组的列表，存储着历史行为和奖励。
```
[(1,8), (2,4), (1,7)]
```
在上面的例子中，有3个样本（我们拉了三个臂）。 我们首先拉开1号臂并获得8美元。 然后我们拉了2手臂并获得了4美元。 对于我们的最后一次行动，我们决定再次拉动第1手，这次我们收到7美元。

提供这个经验数组，我们现在可以创建一个函数来确定为我们提供最高平均回报的手臂，因此应该再次拉动。 在上述情况下，第1组为我们提供了最高的平均回报(8+7)/2=7.5。
```python
def get_best_arm(pastRewards, actions):
    bestArm = 0 #just default to 0
    bestMean = 0
    for action in actions:
        avg = np.mean(pastRewards[np.where(pastRewards[:,0] == action)][:, 1])  # 元组中第一个值是action的所有行的第二个值求均值 
        if avg > bestMean:
            bestMean = avg
            bestArm = action
    return bestArm
```
这是每场比赛的主要循环。 如果随机数大于`epsilon`参数，那么我们只使用`get_best_arm`函数计算最佳动作并采取该动作，否则我们采取随机动作。 我已将其设置为播放500次并显示平均奖励对戏剧的matplotlib散点图。 希望我们会看到平均奖励随着我们玩更多次而增加。

```python
plt.xlabel("Plays")
plt.ylabel("Avg Reward")
for i in range(500):
    if random.random() > eps:
        choice = get_best_arm(pastRewards, actions)
    else:
        choice = np.where(arms == np.random.choice(arms))[0][0]
    thisAV = np.array([[choice, reward(arms[choice])]])
    av = np.vstack((av, thisAV))    
    percCorrect = 100*(len(av[np.where(av[:,0] == np.argmax(arms))])/len(av))
    runningMean = np.mean(av[:,1])
    plt.scatter(i, runningMean)
```
![图片](深度强化学习实战/Figure2.6.png)

正如你所看到的，平均奖励在很多比赛后确实有所改善。我们的算法是学习，它通过以前的好戏得到加强！然而它却是一个如此简单的算法。

我们在这里考虑的问题是一个固定的问题，因为每个手臂的潜在奖励概率分布不随时间而变化。我们当然可以考虑这个问题的变体，这不是真的，一个非平稳的问题。在这种情况下，一个简单的修改就是对比近距离更大的(动作,值)对进行加权，因此如果事情随着时间的推移而变化，我们就能够跟踪它们。除了这个简短的提及，我们不会在这里实现这个稍微复杂的变体。

softmax选择政策。想象一下另一种类型的强盗问题：一位新近出生的医生专门治疗心脏病患者。她有10种治疗方案，她只能选择一种治疗她看到的每位患者。出于某种原因，她所知道的是，这10种治疗方法在治疗心脏病方面具有不同的功效和风险特征，而且她不知道哪种治疗效果最好。我们仍然可以使用上面相同的`ε-贪婪`算法，但是，我们可能想要重新考虑我们的`ε`策略，即在一段时间内完全随机选择治疗。在这个新问题中，随机选择治疗可能导致患者死亡，而不仅仅是失去一些钱。因此，我们确实希望确保不选择最差的治疗方法，但仍然有能力探索我们的选择以找到最好的治疗方案。

这是softmax选择可能最合适的地方。 softmax不仅仅是在探索过程中随机选择一个动作，而是为我们提供了跨越我们选项的概率分布。 具有最大概率的选项将等同于我们从上面的最佳手臂动作，但是我们对例如第二和第三最佳动作有一些了解。 这样，我们可以随机选择探索其他选项，同时避免最糟糕的选择。 这是softmax方程式：

$$P_r(A)=\frac {e^{\frac {Q_k(A)}{\tau}}{\sum_{i=1}^{n}e^{\frac {Q_k(i)}{\tau}}$$

```python
def softmax(vals,tau):
	softm = pow(e,vals/tau)/sum(pow(e,vals/tau))
	return softm
```

$P_r(A)$是接受动作值向量（数组）并返回动作的概率分布的函数，使得较高值的动作具有较高的概率。 例如，如果你的action-value数组有4个可能的动作，并且它们当前都有相同的值，比如A = [10,10,10,10]，则Pr（A）= [0.25,0.25,0.25,0.25] ，即所有概率都相同，必须总和为1。

分数的分子仅对动作值数组除以参数`τ`进行取幂，得到与输入大小相同的向量。分母对每个单独动作值的取幂除以`τ`进行求和，得到一个数。

> `τ`是一个称为温度的参数，用于衡量动作的概率分布。高温会使概率非常相似，而低温则会夸大行动之间概率的差异。选择此参数需要有根据的猜测和一些试验和错误。

当我们使用`softmax`从上面实现老虎机10武装强盗问题时，我们不再需要我们的`get_best_arm`功能了。由于`softmax`在我们可能的动作中产生加权概率分布，我们将根据它们的相对概率随机（但加权）选择动作。也就是说，我们的最佳动作将被更频繁地选择，因为它将具有最高的`softmax`概率，但是将以较低的频率选择其他动作。

为了实现这一点，我们需要创建一个新的numpy数组来存储我们的动作的`softmax`概率。我们将所有动作初始化为具有$\frac {1}{n}$的统一小概率（因为我们有10个动作，每个动作将被赋予0.1的概率）。因此，在我们第一次玩游戏时，所有动作都有相同的被选择概率，但从那时起，最初的统一概率分布将变得高度偏向于产生最高奖励的动作。

```python
n = 10
arms = np.random.rand(n)
av = np.ones(n)
counts = np.zeros(n)
av_softmax = np.zeros(n)
av_softmax[:] = 1.0/n
```
数学指数是numpy中对np.exp（...）的函数调用。它将在输入向量中逐元素地应用函数。
```python
def softmax(av, tau=1.12):
    softm = ( np.exp(av / tau) / np.sum( np.exp(av / tau) ) )
    return softm
```

我们的主要`for`循环现在有点不同。 为了从我们的`softmax`概率分布中随机选择一个动作，我们使用`numpy`的`random.choice`函数。 `random.choice（arr，p = None）`为这些值中的每一个接受值的数组（例如`arr`）和另一个概率数组（例如`p`），然后它将使用`p`中的概率分布随机选择`arr`中的值。 我们将给它`arr = arms`，因此它将随机选择对应于特定动作的索引值，给定`av_softmax`中的概率分布。 我们在前面部分中开发的更新规则保持不变，但是我们需要重新计算存储在`av_softmax`数组中的`softmax`概率，因为我们刚刚采取了一个新的运行平均值。
```python
for i in range(500):
    choice = np.random.choice(arms, p=av_softmax)
    counts[choice] += 1
    k = counts[choice]
    rwd =  reward(arms[choice])
    old_avg = av[choice]
    new_avg = old_avg + (1/k)*(rwd - old_avg)
    av[choice] = new_avg
    av_softmax = softmax(av)
```

![图片](深度强化学习实战/Figure2.7.png)
`Softmax`动作选择似乎比`epsilon-greedy`方法更好; 看起来它更快地收敛于最优政策。 `softmax`的缺点是必须手动选择`τ`参数。 `Softmax`在这里对`τ`非常敏感，需要一段时间才能找到它的好价值。 显然，对于`epsilon-greedy`，我们设置了参数`epsilon`，但选择该参数更加直观。